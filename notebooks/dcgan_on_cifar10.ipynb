{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "id": "vaEQbeFs1Qo4",
    "outputId": "8136761d-2af1-4bb1-c75f-97ae90741b87"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random # to set the python random seed\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import dill as pickle\n",
    "# Ignore excessive warnings\n",
    "import logging\n",
    "logging.propagate = False \n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 42\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "# WandB – Import the wandb library\n",
    "import wandb\n",
    "wandb.login()\n",
    "wandb.init(project=\"dcgan\") # Change the project name based on your W & B account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1PBsFOJ6mbc"
   },
   "source": [
    "## Parameters\n",
    "Note that the Pytorch tutorial [referenced below](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) is designed for the **Celebrity faces** dataset and produces `64 x 64` images. I've tweaked the network architecture to produce `32 x 32` images as corresponding to the **CIFAR-10** dataset. The parameters below reflect the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KCq-VukU6h9E"
   },
   "outputs": [],
   "source": [
    "# Number of workers for dataloader\n",
    "workers = 1\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 32\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ha4uNIH4OXJ"
   },
   "source": [
    "## Model Definition\n",
    "Let's define a generator and discriminator first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7hAFf5Ue4VP_"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HlDt1vdT4jjg"
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "class CIFARGenerator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d( ngf * 2, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bjEH0G1y6w-Q"
   },
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "class CIFARDiscriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c0emU3124y_"
   },
   "source": [
    "## Defining the Training Function\n",
    "For this experiment, we use the CIFAR-10 dataset which has 10 object classes with each image sized at 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6KxHME3e2iGV"
   },
   "outputs": [],
   "source": [
    "def train(args, gen, disc, device, dataloader, optimizerG, optimizerD, criterion, epoch, iters):\n",
    "  gen.train()\n",
    "  disc.train()\n",
    "  img_list = []\n",
    "  fixed_noise = torch.randn(64, config.nz, 1, 1, device=device)\n",
    "\n",
    "  # Establish convention for real and fake labels during training (with label smoothing)\n",
    "  real_label = 0.9\n",
    "  fake_label = 0.1\n",
    "  for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "      #*****\n",
    "      # Update Discriminator\n",
    "      #*****\n",
    "      ## Train with all-real batch\n",
    "      disc.zero_grad()\n",
    "      # Format batch\n",
    "      real_cpu = data[0].to(device)\n",
    "      b_size = real_cpu.size(0)\n",
    "      label = torch.full((b_size,), real_label, device=device)\n",
    "      # Forward pass real batch through D\n",
    "      output = disc(real_cpu).view(-1)\n",
    "      # Calculate loss on all-real batch\n",
    "      errD_real = criterion(output, label)\n",
    "      # Calculate gradients for D in backward pass\n",
    "      errD_real.backward()\n",
    "      D_x = output.mean().item()\n",
    "\n",
    "      ## Train with all-fake batch\n",
    "      # Generate batch of latent vectors\n",
    "      noise = torch.randn(b_size, config.nz, 1, 1, device=device)\n",
    "      # Generate fake image batch with G\n",
    "      fake = gen(noise)\n",
    "      label.fill_(fake_label)\n",
    "      # Classify all fake batch with D\n",
    "      output = disc(fake.detach()).view(-1)\n",
    "      # Calculate D's loss on the all-fake batch\n",
    "      errD_fake = criterion(output, label)\n",
    "      # Calculate the gradients for this batch\n",
    "      errD_fake.backward()\n",
    "      D_G_z1 = output.mean().item()\n",
    "      # Add the gradients from the all-real and all-fake batches\n",
    "      errD = errD_real + errD_fake\n",
    "      # Update D\n",
    "      optimizerD.step()\n",
    "\n",
    "      #*****\n",
    "      # Update Generator\n",
    "      #*****\n",
    "      gen.zero_grad()\n",
    "      label.fill_(real_label)  # fake labels are real for generator cost\n",
    "      # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "      output = disc(fake).view(-1)\n",
    "      # Calculate G's loss based on this output\n",
    "      errG = criterion(output, label)\n",
    "      # Calculate gradients for G\n",
    "      errG.backward()\n",
    "      D_G_z2 = output.mean().item()\n",
    "      # Update G\n",
    "      optimizerG.step()\n",
    "\n",
    "      # Output training stats\n",
    "      if i % 50 == 0:\n",
    "          print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                % (epoch, args.epochs, i, len(dataloader),\n",
    "                    errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "          wandb.log({\n",
    "              \"Gen Loss\": errG.item(),\n",
    "              \"Disc Loss\": errD.item()})\n",
    "\n",
    "      # Check how the generator is doing by saving G's output on fixed_noise\n",
    "      if (iters % 500 == 0) or ((epoch == args.epochs-1) and (i == len(dataloader)-1)):\n",
    "          with torch.no_grad():\n",
    "              fake = gen(fixed_noise).detach().cpu()\n",
    "          img_list.append(wandb.Image(vutils.make_grid(fake, padding=2, normalize=True)))\n",
    "          wandb.log({\n",
    "              \"Generated Images\": img_list})\n",
    "      iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hFb5Oh68R7L"
   },
   "source": [
    "## Monitoring the Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AdBRhaEn8TvZ",
    "outputId": "2c4e23f3-55f6-40a2-d789-de827c6f74d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[1/30][0/391]\tLoss_D: 1.4780\tLoss_G: 2.0657\tD(x): 0.5485\tD(G(z)): 0.5116 / 0.1238\n",
      "[1/30][50/391]\tLoss_D: 0.7573\tLoss_G: 3.9185\tD(x): 0.7646\tD(G(z)): 0.1119 / 0.0146\n",
      "[1/30][100/391]\tLoss_D: 0.7122\tLoss_G: 2.7973\tD(x): 0.8238\tD(G(z)): 0.0982 / 0.0473\n",
      "[1/30][150/391]\tLoss_D: 0.7236\tLoss_G: 2.7978\tD(x): 0.8183\tD(G(z)): 0.0679 / 0.0468\n",
      "[1/30][200/391]\tLoss_D: 0.8709\tLoss_G: 1.8937\tD(x): 0.7193\tD(G(z)): 0.2072 / 0.1365\n",
      "[1/30][250/391]\tLoss_D: 1.0548\tLoss_G: 1.8929\tD(x): 0.5714\tD(G(z)): 0.1678 / 0.1453\n",
      "[1/30][300/391]\tLoss_D: 1.2422\tLoss_G: 2.3039\tD(x): 0.5433\tD(G(z)): 0.3590 / 0.0934\n",
      "[1/30][350/391]\tLoss_D: 0.9516\tLoss_G: 2.7085\tD(x): 0.7582\tD(G(z)): 0.3455 / 0.0582\n",
      "[2/30][0/391]\tLoss_D: 1.1176\tLoss_G: 2.0763\tD(x): 0.7484\tD(G(z)): 0.4235 / 0.1174\n",
      "[2/30][50/391]\tLoss_D: 0.8758\tLoss_G: 2.1330\tD(x): 0.7424\tD(G(z)): 0.2689 / 0.1052\n",
      "[2/30][100/391]\tLoss_D: 1.0533\tLoss_G: 1.9892\tD(x): 0.6541\tD(G(z)): 0.3193 / 0.1354\n",
      "[2/30][150/391]\tLoss_D: 0.8182\tLoss_G: 2.5719\tD(x): 0.7806\tD(G(z)): 0.2231 / 0.0681\n",
      "[2/30][200/391]\tLoss_D: 1.0619\tLoss_G: 2.1192\tD(x): 0.6759\tD(G(z)): 0.3348 / 0.1151\n",
      "[2/30][250/391]\tLoss_D: 1.0286\tLoss_G: 1.7972\tD(x): 0.6515\tD(G(z)): 0.3117 / 0.1638\n",
      "[2/30][300/391]\tLoss_D: 0.8450\tLoss_G: 2.0991\tD(x): 0.7678\tD(G(z)): 0.2569 / 0.1130\n",
      "[2/30][350/391]\tLoss_D: 1.0457\tLoss_G: 1.8579\tD(x): 0.7041\tD(G(z)): 0.3496 / 0.1713\n",
      "[3/30][0/391]\tLoss_D: 0.8397\tLoss_G: 2.0470\tD(x): 0.7040\tD(G(z)): 0.1902 / 0.1164\n",
      "[3/30][50/391]\tLoss_D: 0.9711\tLoss_G: 2.3974\tD(x): 0.8025\tD(G(z)): 0.4004 / 0.0840\n",
      "[3/30][100/391]\tLoss_D: 0.9330\tLoss_G: 2.5218\tD(x): 0.8385\tD(G(z)): 0.3905 / 0.0703\n",
      "[3/30][150/391]\tLoss_D: 0.8753\tLoss_G: 1.8782\tD(x): 0.7030\tD(G(z)): 0.2159 / 0.1472\n",
      "[3/30][200/391]\tLoss_D: 1.0416\tLoss_G: 1.8732\tD(x): 0.6865\tD(G(z)): 0.3581 / 0.1481\n",
      "[3/30][250/391]\tLoss_D: 0.8936\tLoss_G: 1.6339\tD(x): 0.6757\tD(G(z)): 0.2061 / 0.1838\n",
      "[3/30][300/391]\tLoss_D: 0.9736\tLoss_G: 1.6194\tD(x): 0.6493\tD(G(z)): 0.2515 / 0.1904\n",
      "[3/30][350/391]\tLoss_D: 0.9350\tLoss_G: 2.5771\tD(x): 0.7868\tD(G(z)): 0.3574 / 0.0691\n",
      "[4/30][0/391]\tLoss_D: 0.9618\tLoss_G: 1.3512\tD(x): 0.6443\tD(G(z)): 0.2372 / 0.2613\n",
      "[4/30][50/391]\tLoss_D: 0.9111\tLoss_G: 1.9951\tD(x): 0.7870\tD(G(z)): 0.3401 / 0.1270\n",
      "[4/30][100/391]\tLoss_D: 0.9324\tLoss_G: 1.8639\tD(x): 0.7181\tD(G(z)): 0.3001 / 0.1479\n",
      "[4/30][150/391]\tLoss_D: 0.8162\tLoss_G: 1.8020\tD(x): 0.7676\tD(G(z)): 0.2163 / 0.1575\n",
      "[4/30][200/391]\tLoss_D: 0.8628\tLoss_G: 1.5416\tD(x): 0.7026\tD(G(z)): 0.2074 / 0.2010\n",
      "[4/30][250/391]\tLoss_D: 0.8177\tLoss_G: 1.7280\tD(x): 0.7250\tD(G(z)): 0.1794 / 0.1695\n",
      "[4/30][300/391]\tLoss_D: 0.8358\tLoss_G: 1.9834\tD(x): 0.7409\tD(G(z)): 0.2206 / 0.1269\n",
      "[4/30][350/391]\tLoss_D: 0.8234\tLoss_G: 1.9736\tD(x): 0.7008\tD(G(z)): 0.1083 / 0.1271\n",
      "[5/30][0/391]\tLoss_D: 1.0791\tLoss_G: 3.2411\tD(x): 0.8112\tD(G(z)): 0.4744 / 0.0369\n",
      "[5/30][50/391]\tLoss_D: 0.7654\tLoss_G: 1.9981\tD(x): 0.8024\tD(G(z)): 0.1628 / 0.1286\n",
      "[5/30][100/391]\tLoss_D: 0.8490\tLoss_G: 1.8940\tD(x): 0.8192\tD(G(z)): 0.3075 / 0.1379\n",
      "[5/30][150/391]\tLoss_D: 0.9198\tLoss_G: 1.8512\tD(x): 0.7962\tD(G(z)): 0.3548 / 0.1488\n",
      "[5/30][200/391]\tLoss_D: 1.1266\tLoss_G: 2.7698\tD(x): 0.8571\tD(G(z)): 0.5131 / 0.0600\n",
      "[5/30][250/391]\tLoss_D: 1.0783\tLoss_G: 1.3235\tD(x): 0.5587\tD(G(z)): 0.2304 / 0.2692\n",
      "[5/30][300/391]\tLoss_D: 1.0576\tLoss_G: 2.1677\tD(x): 0.7321\tD(G(z)): 0.3905 / 0.1079\n",
      "[5/30][350/391]\tLoss_D: 1.0176\tLoss_G: 1.4413\tD(x): 0.6075\tD(G(z)): 0.2358 / 0.2341\n",
      "[6/30][0/391]\tLoss_D: 0.9292\tLoss_G: 1.9662\tD(x): 0.7683\tD(G(z)): 0.3480 / 0.1313\n",
      "[6/30][50/391]\tLoss_D: 1.0966\tLoss_G: 1.1427\tD(x): 0.5424\tD(G(z)): 0.2005 / 0.3264\n",
      "[6/30][100/391]\tLoss_D: 1.0404\tLoss_G: 1.4653\tD(x): 0.6716\tD(G(z)): 0.3667 / 0.2210\n",
      "[6/30][150/391]\tLoss_D: 0.9998\tLoss_G: 1.5165\tD(x): 0.6499\tD(G(z)): 0.2970 / 0.2141\n",
      "[6/30][200/391]\tLoss_D: 1.0872\tLoss_G: 1.2723\tD(x): 0.6420\tD(G(z)): 0.3810 / 0.2765\n",
      "[6/30][250/391]\tLoss_D: 0.9639\tLoss_G: 1.5171\tD(x): 0.7198\tD(G(z)): 0.3405 / 0.2104\n",
      "[6/30][300/391]\tLoss_D: 1.0108\tLoss_G: 1.1646\tD(x): 0.5823\tD(G(z)): 0.2323 / 0.3090\n",
      "[6/30][350/391]\tLoss_D: 1.0684\tLoss_G: 1.6076\tD(x): 0.6431\tD(G(z)): 0.3543 / 0.1866\n",
      "[7/30][0/391]\tLoss_D: 0.9726\tLoss_G: 1.5694\tD(x): 0.6582\tD(G(z)): 0.2936 / 0.2022\n",
      "[7/30][50/391]\tLoss_D: 1.0774\tLoss_G: 1.3212\tD(x): 0.6104\tD(G(z)): 0.3283 / 0.2627\n",
      "[7/30][100/391]\tLoss_D: 1.3159\tLoss_G: 0.9466\tD(x): 0.4520\tD(G(z)): 0.3093 / 0.3906\n",
      "[7/30][150/391]\tLoss_D: 0.9812\tLoss_G: 1.4631\tD(x): 0.6666\tD(G(z)): 0.2992 / 0.2200\n",
      "[7/30][200/391]\tLoss_D: 0.9149\tLoss_G: 2.0480\tD(x): 0.7980\tD(G(z)): 0.3523 / 0.1171\n",
      "[7/30][250/391]\tLoss_D: 1.1961\tLoss_G: 1.7750\tD(x): 0.7717\tD(G(z)): 0.5294 / 0.1660\n",
      "[7/30][300/391]\tLoss_D: 1.1057\tLoss_G: 1.9456\tD(x): 0.7700\tD(G(z)): 0.4871 / 0.1288\n",
      "[7/30][350/391]\tLoss_D: 1.0300\tLoss_G: 1.8775\tD(x): 0.7120\tD(G(z)): 0.3851 / 0.1419\n",
      "[8/30][0/391]\tLoss_D: 1.0372\tLoss_G: 1.0525\tD(x): 0.5516\tD(G(z)): 0.2001 / 0.3478\n",
      "[8/30][50/391]\tLoss_D: 1.2659\tLoss_G: 0.7850\tD(x): 0.4457\tD(G(z)): 0.2612 / 0.4725\n",
      "[8/30][100/391]\tLoss_D: 1.0936\tLoss_G: 1.1700\tD(x): 0.5250\tD(G(z)): 0.2019 / 0.3113\n",
      "[8/30][150/391]\tLoss_D: 1.1760\tLoss_G: 1.5404\tD(x): 0.7668\tD(G(z)): 0.5102 / 0.2093\n",
      "[8/30][200/391]\tLoss_D: 1.1015\tLoss_G: 1.3304\tD(x): 0.6032\tD(G(z)): 0.3422 / 0.2596\n",
      "[8/30][250/391]\tLoss_D: 1.0470\tLoss_G: 1.9363\tD(x): 0.7708\tD(G(z)): 0.4469 / 0.1321\n",
      "[8/30][300/391]\tLoss_D: 1.0801\tLoss_G: 1.3816\tD(x): 0.6405\tD(G(z)): 0.3763 / 0.2406\n",
      "[8/30][350/391]\tLoss_D: 1.1779\tLoss_G: 1.6350\tD(x): 0.6771\tD(G(z)): 0.4662 / 0.1845\n",
      "[9/30][0/391]\tLoss_D: 1.0686\tLoss_G: 1.4099\tD(x): 0.5631\tD(G(z)): 0.2624 / 0.2384\n",
      "[9/30][50/391]\tLoss_D: 0.9860\tLoss_G: 1.3646\tD(x): 0.6191\tD(G(z)): 0.2536 / 0.2538\n",
      "[9/30][100/391]\tLoss_D: 0.9939\tLoss_G: 1.4188\tD(x): 0.6913\tD(G(z)): 0.3429 / 0.2296\n",
      "[9/30][150/391]\tLoss_D: 1.0935\tLoss_G: 1.2411\tD(x): 0.5707\tD(G(z)): 0.3107 / 0.2802\n",
      "[9/30][200/391]\tLoss_D: 1.1406\tLoss_G: 1.7538\tD(x): 0.6716\tD(G(z)): 0.4359 / 0.1632\n",
      "[9/30][250/391]\tLoss_D: 1.2267\tLoss_G: 1.5237\tD(x): 0.7087\tD(G(z)): 0.5182 / 0.2093\n",
      "[9/30][300/391]\tLoss_D: 1.0799\tLoss_G: 1.5954\tD(x): 0.6986\tD(G(z)): 0.4240 / 0.1914\n",
      "[9/30][350/391]\tLoss_D: 1.1505\tLoss_G: 1.4681\tD(x): 0.6786\tD(G(z)): 0.4612 / 0.2181\n",
      "[10/30][0/391]\tLoss_D: 1.0041\tLoss_G: 1.6394\tD(x): 0.7167\tD(G(z)): 0.3755 / 0.1811\n",
      "[10/30][50/391]\tLoss_D: 0.9779\tLoss_G: 1.1834\tD(x): 0.6266\tD(G(z)): 0.2610 / 0.3003\n",
      "[10/30][100/391]\tLoss_D: 1.1067\tLoss_G: 1.0002\tD(x): 0.5701\tD(G(z)): 0.3255 / 0.3637\n",
      "[10/30][150/391]\tLoss_D: 1.0742\tLoss_G: 1.1929\tD(x): 0.6552\tD(G(z)): 0.3837 / 0.2948\n",
      "[10/30][200/391]\tLoss_D: 0.9689\tLoss_G: 1.5094\tD(x): 0.7211\tD(G(z)): 0.3523 / 0.2112\n",
      "[10/30][250/391]\tLoss_D: 1.0392\tLoss_G: 0.8749\tD(x): 0.5768\tD(G(z)): 0.2456 / 0.4194\n",
      "[10/30][300/391]\tLoss_D: 1.0877\tLoss_G: 1.4618\tD(x): 0.7030\tD(G(z)): 0.4265 / 0.2260\n",
      "[10/30][350/391]\tLoss_D: 1.1429\tLoss_G: 1.0304\tD(x): 0.5121\tD(G(z)): 0.2784 / 0.3547\n",
      "[11/30][0/391]\tLoss_D: 1.0880\tLoss_G: 0.9290\tD(x): 0.5858\tD(G(z)): 0.3189 / 0.3921\n",
      "[11/30][50/391]\tLoss_D: 1.2508\tLoss_G: 1.8414\tD(x): 0.8075\tD(G(z)): 0.5771 / 0.1460\n",
      "[11/30][100/391]\tLoss_D: 1.0847\tLoss_G: 1.5396\tD(x): 0.6824\tD(G(z)): 0.4236 / 0.2028\n",
      "[11/30][150/391]\tLoss_D: 1.0093\tLoss_G: 1.4310\tD(x): 0.6766\tD(G(z)): 0.3499 / 0.2284\n",
      "[11/30][200/391]\tLoss_D: 1.0066\tLoss_G: 1.3310\tD(x): 0.7090\tD(G(z)): 0.3758 / 0.2555\n",
      "[11/30][250/391]\tLoss_D: 1.0279\tLoss_G: 1.3228\tD(x): 0.5978\tD(G(z)): 0.2796 / 0.2578\n",
      "[11/30][300/391]\tLoss_D: 1.0150\tLoss_G: 1.5752\tD(x): 0.7147\tD(G(z)): 0.3909 / 0.1941\n",
      "[11/30][350/391]\tLoss_D: 1.0324\tLoss_G: 1.5022\tD(x): 0.6897\tD(G(z)): 0.3801 / 0.2128\n",
      "[12/30][0/391]\tLoss_D: 1.0576\tLoss_G: 1.1511\tD(x): 0.5990\tD(G(z)): 0.3156 / 0.3068\n",
      "[12/30][50/391]\tLoss_D: 1.0988\tLoss_G: 1.3262\tD(x): 0.6705\tD(G(z)): 0.4180 / 0.2514\n",
      "[12/30][100/391]\tLoss_D: 1.1768\tLoss_G: 0.9943\tD(x): 0.4897\tD(G(z)): 0.2493 / 0.3676\n",
      "[12/30][150/391]\tLoss_D: 1.2360\tLoss_G: 1.3288\tD(x): 0.6868\tD(G(z)): 0.5090 / 0.2581\n",
      "[12/30][200/391]\tLoss_D: 1.0283\tLoss_G: 1.0954\tD(x): 0.5987\tD(G(z)): 0.2794 / 0.3311\n",
      "[12/30][250/391]\tLoss_D: 1.2089\tLoss_G: 1.4648\tD(x): 0.7249\tD(G(z)): 0.5269 / 0.2194\n",
      "[12/30][300/391]\tLoss_D: 1.1362\tLoss_G: 0.9063\tD(x): 0.4836\tD(G(z)): 0.1768 / 0.4085\n",
      "[12/30][350/391]\tLoss_D: 0.9164\tLoss_G: 1.4247\tD(x): 0.7002\tD(G(z)): 0.2926 / 0.2287\n",
      "[13/30][0/391]\tLoss_D: 1.1215\tLoss_G: 1.1960\tD(x): 0.6581\tD(G(z)): 0.4203 / 0.2952\n",
      "[13/30][50/391]\tLoss_D: 1.0772\tLoss_G: 1.1853\tD(x): 0.6143\tD(G(z)): 0.3480 / 0.2978\n",
      "[13/30][100/391]\tLoss_D: 1.0898\tLoss_G: 1.3477\tD(x): 0.6961\tD(G(z)): 0.4342 / 0.2460\n",
      "[13/30][150/391]\tLoss_D: 1.0372\tLoss_G: 1.4258\tD(x): 0.7061\tD(G(z)): 0.3990 / 0.2285\n",
      "[13/30][200/391]\tLoss_D: 1.0405\tLoss_G: 1.3444\tD(x): 0.7087\tD(G(z)): 0.4054 / 0.2486\n",
      "[13/30][250/391]\tLoss_D: 1.1352\tLoss_G: 0.9935\tD(x): 0.5330\tD(G(z)): 0.3000 / 0.3652\n",
      "[13/30][300/391]\tLoss_D: 1.3945\tLoss_G: 0.7329\tD(x): 0.3592\tD(G(z)): 0.1258 / 0.5030\n",
      "[13/30][350/391]\tLoss_D: 1.2009\tLoss_G: 1.0180\tD(x): 0.4834\tD(G(z)): 0.2472 / 0.3652\n",
      "[14/30][0/391]\tLoss_D: 1.0153\tLoss_G: 1.2463\tD(x): 0.6452\tD(G(z)): 0.3253 / 0.2761\n",
      "[14/30][50/391]\tLoss_D: 1.1230\tLoss_G: 1.1640\tD(x): 0.5807\tD(G(z)): 0.3548 / 0.3036\n",
      "[14/30][100/391]\tLoss_D: 1.0718\tLoss_G: 1.6335\tD(x): 0.6144\tD(G(z)): 0.3374 / 0.1864\n",
      "[14/30][150/391]\tLoss_D: 1.0105\tLoss_G: 1.2567\tD(x): 0.5626\tD(G(z)): 0.1954 / 0.2778\n",
      "[14/30][200/391]\tLoss_D: 0.9400\tLoss_G: 1.6728\tD(x): 0.7417\tD(G(z)): 0.3475 / 0.1795\n",
      "[14/30][250/391]\tLoss_D: 1.0962\tLoss_G: 1.1618\tD(x): 0.6136\tD(G(z)): 0.3544 / 0.3109\n",
      "[14/30][300/391]\tLoss_D: 1.0592\tLoss_G: 1.1288\tD(x): 0.5738\tD(G(z)): 0.2683 / 0.3182\n",
      "[14/30][350/391]\tLoss_D: 1.2903\tLoss_G: 1.5651\tD(x): 0.6924\tD(G(z)): 0.5564 / 0.1933\n",
      "[15/30][0/391]\tLoss_D: 1.2237\tLoss_G: 1.9518\tD(x): 0.7694\tD(G(z)): 0.5507 / 0.1329\n",
      "[15/30][50/391]\tLoss_D: 1.0657\tLoss_G: 1.7276\tD(x): 0.7246\tD(G(z)): 0.4254 / 0.1667\n",
      "[15/30][100/391]\tLoss_D: 1.0841\tLoss_G: 1.6120\tD(x): 0.7064\tD(G(z)): 0.4327 / 0.1859\n",
      "[15/30][150/391]\tLoss_D: 1.0214\tLoss_G: 1.1653\tD(x): 0.6877\tD(G(z)): 0.3738 / 0.3054\n",
      "[15/30][200/391]\tLoss_D: 1.1083\tLoss_G: 1.0786\tD(x): 0.5645\tD(G(z)): 0.3173 / 0.3433\n",
      "[15/30][250/391]\tLoss_D: 1.0617\tLoss_G: 1.1946\tD(x): 0.5787\tD(G(z)): 0.2789 / 0.3029\n",
      "[15/30][300/391]\tLoss_D: 1.3920\tLoss_G: 1.9578\tD(x): 0.8052\tD(G(z)): 0.6307 / 0.1321\n",
      "[15/30][350/391]\tLoss_D: 1.1795\tLoss_G: 0.9739\tD(x): 0.4896\tD(G(z)): 0.2554 / 0.3795\n",
      "[16/30][0/391]\tLoss_D: 1.1072\tLoss_G: 1.1179\tD(x): 0.5950\tD(G(z)): 0.3512 / 0.3203\n",
      "[16/30][50/391]\tLoss_D: 1.1147\tLoss_G: 0.9047\tD(x): 0.5743\tD(G(z)): 0.3427 / 0.4045\n",
      "[16/30][100/391]\tLoss_D: 0.9705\tLoss_G: 1.4828\tD(x): 0.6417\tD(G(z)): 0.2854 / 0.2151\n",
      "[16/30][150/391]\tLoss_D: 1.0615\tLoss_G: 1.6347\tD(x): 0.7884\tD(G(z)): 0.4704 / 0.1802\n",
      "[16/30][200/391]\tLoss_D: 1.1259\tLoss_G: 1.2158\tD(x): 0.5879\tD(G(z)): 0.3654 / 0.2884\n",
      "[16/30][250/391]\tLoss_D: 1.1221\tLoss_G: 1.1748\tD(x): 0.5797\tD(G(z)): 0.3338 / 0.3022\n",
      "[16/30][300/391]\tLoss_D: 1.2552\tLoss_G: 1.8928\tD(x): 0.7706\tD(G(z)): 0.5660 / 0.1415\n",
      "[16/30][350/391]\tLoss_D: 1.0838\tLoss_G: 1.1386\tD(x): 0.5852\tD(G(z)): 0.3234 / 0.3105\n",
      "[17/30][0/391]\tLoss_D: 1.0659\tLoss_G: 1.5220\tD(x): 0.7349\tD(G(z)): 0.4338 / 0.2035\n",
      "[17/30][50/391]\tLoss_D: 1.1418\tLoss_G: 1.4476\tD(x): 0.6918\tD(G(z)): 0.4545 / 0.2326\n",
      "[17/30][100/391]\tLoss_D: 1.1592\tLoss_G: 1.5068\tD(x): 0.7321\tD(G(z)): 0.4969 / 0.2089\n",
      "[17/30][150/391]\tLoss_D: 1.1265\tLoss_G: 0.7816\tD(x): 0.5240\tD(G(z)): 0.2741 / 0.4659\n",
      "[17/30][200/391]\tLoss_D: 1.1135\tLoss_G: 1.0807\tD(x): 0.5800\tD(G(z)): 0.3389 / 0.3363\n",
      "[17/30][250/391]\tLoss_D: 1.1602\tLoss_G: 1.3696\tD(x): 0.6147\tD(G(z)): 0.4064 / 0.2448\n",
      "[17/30][300/391]\tLoss_D: 1.1459\tLoss_G: 1.1994\tD(x): 0.6870\tD(G(z)): 0.4599 / 0.2925\n",
      "[17/30][350/391]\tLoss_D: 1.2159\tLoss_G: 1.0358\tD(x): 0.5836\tD(G(z)): 0.4187 / 0.3453\n",
      "[18/30][0/391]\tLoss_D: 1.2712\tLoss_G: 2.1647\tD(x): 0.8147\tD(G(z)): 0.5851 / 0.1092\n",
      "[18/30][50/391]\tLoss_D: 1.1529\tLoss_G: 1.3829\tD(x): 0.6637\tD(G(z)): 0.4458 / 0.2389\n",
      "[18/30][100/391]\tLoss_D: 1.2025\tLoss_G: 1.1747\tD(x): 0.4598\tD(G(z)): 0.2211 / 0.3094\n",
      "[18/30][150/391]\tLoss_D: 1.0491\tLoss_G: 1.1103\tD(x): 0.6320\tD(G(z)): 0.3499 / 0.3189\n",
      "[18/30][200/391]\tLoss_D: 0.9803\tLoss_G: 1.1901\tD(x): 0.6797\tD(G(z)): 0.3358 / 0.2934\n",
      "[18/30][250/391]\tLoss_D: 1.1013\tLoss_G: 1.5740\tD(x): 0.7576\tD(G(z)): 0.4757 / 0.1958\n",
      "[18/30][300/391]\tLoss_D: 1.1963\tLoss_G: 1.7280\tD(x): 0.7790\tD(G(z)): 0.5358 / 0.1678\n",
      "[18/30][350/391]\tLoss_D: 1.2277\tLoss_G: 1.7625\tD(x): 0.7867\tD(G(z)): 0.5625 / 0.1609\n",
      "[19/30][0/391]\tLoss_D: 1.1415\tLoss_G: 1.0541\tD(x): 0.5945\tD(G(z)): 0.3824 / 0.3455\n",
      "[19/30][50/391]\tLoss_D: 1.1191\tLoss_G: 1.3812\tD(x): 0.6778\tD(G(z)): 0.4417 / 0.2378\n",
      "[19/30][100/391]\tLoss_D: 1.0510\tLoss_G: 1.2217\tD(x): 0.6697\tD(G(z)): 0.3884 / 0.2844\n",
      "[19/30][150/391]\tLoss_D: 1.1708\tLoss_G: 0.9353\tD(x): 0.5212\tD(G(z)): 0.3108 / 0.3935\n",
      "[19/30][200/391]\tLoss_D: 1.2693\tLoss_G: 0.9510\tD(x): 0.4632\tD(G(z)): 0.3116 / 0.3859\n",
      "[19/30][250/391]\tLoss_D: 1.2055\tLoss_G: 1.3763\tD(x): 0.5833\tD(G(z)): 0.4161 / 0.2418\n",
      "[19/30][300/391]\tLoss_D: 1.4498\tLoss_G: 1.5207\tD(x): 0.7353\tD(G(z)): 0.6385 / 0.2104\n",
      "[19/30][350/391]\tLoss_D: 1.1145\tLoss_G: 1.1205\tD(x): 0.6437\tD(G(z)): 0.4176 / 0.3203\n",
      "[20/30][0/391]\tLoss_D: 1.0830\tLoss_G: 1.1285\tD(x): 0.6253\tD(G(z)): 0.3663 / 0.3158\n",
      "[20/30][50/391]\tLoss_D: 1.0153\tLoss_G: 1.1420\tD(x): 0.6334\tD(G(z)): 0.3163 / 0.3077\n",
      "[20/30][100/391]\tLoss_D: 0.9876\tLoss_G: 1.2765\tD(x): 0.6357\tD(G(z)): 0.2919 / 0.2731\n",
      "[20/30][150/391]\tLoss_D: 1.1006\tLoss_G: 1.2605\tD(x): 0.6107\tD(G(z)): 0.3659 / 0.2700\n",
      "[20/30][200/391]\tLoss_D: 1.2158\tLoss_G: 1.0986\tD(x): 0.6027\tD(G(z)): 0.4409 / 0.3268\n",
      "[20/30][250/391]\tLoss_D: 1.4074\tLoss_G: 0.6845\tD(x): 0.3579\tD(G(z)): 0.1829 / 0.5245\n",
      "[20/30][300/391]\tLoss_D: 1.0868\tLoss_G: 0.9610\tD(x): 0.5935\tD(G(z)): 0.3400 / 0.3774\n",
      "[20/30][350/391]\tLoss_D: 1.0501\tLoss_G: 1.3590\tD(x): 0.6876\tD(G(z)): 0.4039 / 0.2435\n",
      "[21/30][0/391]\tLoss_D: 1.0452\tLoss_G: 1.3983\tD(x): 0.6685\tD(G(z)): 0.3765 / 0.2369\n",
      "[21/30][50/391]\tLoss_D: 1.0598\tLoss_G: 1.0812\tD(x): 0.5577\tD(G(z)): 0.2474 / 0.3316\n",
      "[21/30][100/391]\tLoss_D: 1.1246\tLoss_G: 0.9582\tD(x): 0.5654\tD(G(z)): 0.3286 / 0.3833\n",
      "[21/30][150/391]\tLoss_D: 1.0796\tLoss_G: 1.2058\tD(x): 0.6397\tD(G(z)): 0.3787 / 0.2907\n",
      "[21/30][200/391]\tLoss_D: 1.0796\tLoss_G: 1.1671\tD(x): 0.5984\tD(G(z)): 0.3392 / 0.3061\n",
      "[21/30][250/391]\tLoss_D: 1.0506\tLoss_G: 1.0796\tD(x): 0.6209\tD(G(z)): 0.3345 / 0.3306\n",
      "[21/30][300/391]\tLoss_D: 1.0953\tLoss_G: 1.3980\tD(x): 0.7163\tD(G(z)): 0.4472 / 0.2354\n",
      "[21/30][350/391]\tLoss_D: 1.1132\tLoss_G: 1.2963\tD(x): 0.6303\tD(G(z)): 0.3935 / 0.2649\n",
      "[22/30][0/391]\tLoss_D: 1.1194\tLoss_G: 0.9618\tD(x): 0.5551\tD(G(z)): 0.3096 / 0.3860\n",
      "[22/30][50/391]\tLoss_D: 1.2572\tLoss_G: 0.7733\tD(x): 0.4296\tD(G(z)): 0.2110 / 0.4709\n",
      "[22/30][100/391]\tLoss_D: 1.1392\tLoss_G: 1.5561\tD(x): 0.7407\tD(G(z)): 0.4931 / 0.1983\n",
      "[22/30][150/391]\tLoss_D: 1.0236\tLoss_G: 1.0127\tD(x): 0.5855\tD(G(z)): 0.2556 / 0.3604\n",
      "[22/30][200/391]\tLoss_D: 1.2184\tLoss_G: 0.6576\tD(x): 0.4457\tD(G(z)): 0.1901 / 0.5450\n",
      "[22/30][250/391]\tLoss_D: 1.1403\tLoss_G: 1.3175\tD(x): 0.6431\tD(G(z)): 0.4244 / 0.2566\n",
      "[22/30][300/391]\tLoss_D: 1.1124\tLoss_G: 1.1013\tD(x): 0.5510\tD(G(z)): 0.3019 / 0.3240\n",
      "[22/30][350/391]\tLoss_D: 1.2718\tLoss_G: 0.8091\tD(x): 0.4725\tD(G(z)): 0.3310 / 0.4518\n",
      "[23/30][0/391]\tLoss_D: 1.0744\tLoss_G: 1.2082\tD(x): 0.6229\tD(G(z)): 0.3596 / 0.2877\n",
      "[23/30][50/391]\tLoss_D: 1.1377\tLoss_G: 1.0390\tD(x): 0.5760\tD(G(z)): 0.3488 / 0.3529\n",
      "[23/30][100/391]\tLoss_D: 1.1034\tLoss_G: 1.1017\tD(x): 0.6029\tD(G(z)): 0.3591 / 0.3201\n",
      "[23/30][150/391]\tLoss_D: 1.2122\tLoss_G: 1.4038\tD(x): 0.7153\tD(G(z)): 0.5093 / 0.2387\n",
      "[23/30][200/391]\tLoss_D: 1.0641\tLoss_G: 1.2418\tD(x): 0.7068\tD(G(z)): 0.4150 / 0.2800\n",
      "[23/30][250/391]\tLoss_D: 1.4392\tLoss_G: 0.6987\tD(x): 0.3764\tD(G(z)): 0.2829 / 0.5184\n",
      "[23/30][300/391]\tLoss_D: 1.1267\tLoss_G: 1.3277\tD(x): 0.5742\tD(G(z)): 0.3554 / 0.2502\n",
      "[23/30][350/391]\tLoss_D: 1.2023\tLoss_G: 0.9668\tD(x): 0.5022\tD(G(z)): 0.3152 / 0.3822\n",
      "[24/30][0/391]\tLoss_D: 1.1571\tLoss_G: 1.0175\tD(x): 0.5321\tD(G(z)): 0.3190 / 0.3586\n",
      "[24/30][50/391]\tLoss_D: 1.1240\tLoss_G: 1.0125\tD(x): 0.5581\tD(G(z)): 0.3175 / 0.3586\n",
      "[24/30][100/391]\tLoss_D: 1.1018\tLoss_G: 1.2159\tD(x): 0.6359\tD(G(z)): 0.3880 / 0.2896\n",
      "[24/30][150/391]\tLoss_D: 1.1395\tLoss_G: 1.2370\tD(x): 0.6209\tD(G(z)): 0.3994 / 0.2810\n",
      "[24/30][200/391]\tLoss_D: 0.9936\tLoss_G: 1.3585\tD(x): 0.6914\tD(G(z)): 0.3571 / 0.2460\n",
      "[24/30][250/391]\tLoss_D: 1.1888\tLoss_G: 1.2968\tD(x): 0.6304\tD(G(z)): 0.4462 / 0.2656\n",
      "[24/30][300/391]\tLoss_D: 1.0979\tLoss_G: 1.6153\tD(x): 0.7489\tD(G(z)): 0.4721 / 0.1866\n",
      "[24/30][350/391]\tLoss_D: 1.1749\tLoss_G: 1.1522\tD(x): 0.6566\tD(G(z)): 0.4636 / 0.3037\n",
      "[25/30][0/391]\tLoss_D: 1.1568\tLoss_G: 1.4183\tD(x): 0.6449\tD(G(z)): 0.4321 / 0.2389\n",
      "[25/30][50/391]\tLoss_D: 1.0018\tLoss_G: 1.1742\tD(x): 0.6152\tD(G(z)): 0.2800 / 0.3022\n",
      "[25/30][100/391]\tLoss_D: 1.0895\tLoss_G: 0.9790\tD(x): 0.5586\tD(G(z)): 0.2965 / 0.3702\n",
      "[25/30][150/391]\tLoss_D: 1.1605\tLoss_G: 1.2143\tD(x): 0.6489\tD(G(z)): 0.4330 / 0.2890\n",
      "[25/30][200/391]\tLoss_D: 1.1169\tLoss_G: 1.2815\tD(x): 0.6928\tD(G(z)): 0.4437 / 0.2671\n",
      "[25/30][250/391]\tLoss_D: 1.0827\tLoss_G: 1.3007\tD(x): 0.6174\tD(G(z)): 0.3626 / 0.2620\n",
      "[25/30][300/391]\tLoss_D: 1.1775\tLoss_G: 1.7795\tD(x): 0.7645\tD(G(z)): 0.5211 / 0.1585\n",
      "[25/30][350/391]\tLoss_D: 1.1071\tLoss_G: 0.9788\tD(x): 0.5284\tD(G(z)): 0.2546 / 0.3706\n",
      "[26/30][0/391]\tLoss_D: 1.1235\tLoss_G: 1.7627\tD(x): 0.7954\tD(G(z)): 0.5076 / 0.1583\n",
      "[26/30][50/391]\tLoss_D: 1.0298\tLoss_G: 1.1477\tD(x): 0.6125\tD(G(z)): 0.3044 / 0.3107\n",
      "[26/30][100/391]\tLoss_D: 0.9743\tLoss_G: 1.4065\tD(x): 0.7264\tD(G(z)): 0.3640 / 0.2359\n",
      "[26/30][150/391]\tLoss_D: 1.1123\tLoss_G: 1.0084\tD(x): 0.5208\tD(G(z)): 0.2340 / 0.3614\n",
      "[26/30][200/391]\tLoss_D: 1.0824\tLoss_G: 1.0139\tD(x): 0.6281\tD(G(z)): 0.3592 / 0.3630\n",
      "[26/30][250/391]\tLoss_D: 1.1111\tLoss_G: 0.9263\tD(x): 0.5026\tD(G(z)): 0.2088 / 0.3937\n",
      "[26/30][300/391]\tLoss_D: 1.1756\tLoss_G: 1.4460\tD(x): 0.6920\tD(G(z)): 0.4817 / 0.2287\n",
      "[26/30][350/391]\tLoss_D: 1.0684\tLoss_G: 1.4040\tD(x): 0.6882\tD(G(z)): 0.4079 / 0.2376\n",
      "[27/30][0/391]\tLoss_D: 1.0200\tLoss_G: 1.4084\tD(x): 0.7110\tD(G(z)): 0.3824 / 0.2399\n",
      "[27/30][50/391]\tLoss_D: 1.0438\tLoss_G: 1.3523\tD(x): 0.6575\tD(G(z)): 0.3632 / 0.2446\n",
      "[27/30][100/391]\tLoss_D: 0.9763\tLoss_G: 1.3641\tD(x): 0.6976\tD(G(z)): 0.3500 / 0.2420\n",
      "[27/30][150/391]\tLoss_D: 2.7929\tLoss_G: 3.1718\tD(x): 0.9623\tD(G(z)): 0.9204 / 0.0413\n",
      "[27/30][200/391]\tLoss_D: 1.1603\tLoss_G: 1.0987\tD(x): 0.6087\tD(G(z)): 0.4163 / 0.3253\n",
      "[27/30][250/391]\tLoss_D: 1.0590\tLoss_G: 1.1880\tD(x): 0.5983\tD(G(z)): 0.3160 / 0.2976\n",
      "[27/30][300/391]\tLoss_D: 1.0163\tLoss_G: 1.1724\tD(x): 0.5935\tD(G(z)): 0.2695 / 0.2992\n",
      "[27/30][350/391]\tLoss_D: 1.4302\tLoss_G: 1.9403\tD(x): 0.7580\tD(G(z)): 0.6352 / 0.1349\n",
      "[28/30][0/391]\tLoss_D: 1.1202\tLoss_G: 1.0398\tD(x): 0.5797\tD(G(z)): 0.3426 / 0.3466\n",
      "[28/30][50/391]\tLoss_D: 1.1020\tLoss_G: 1.1874\tD(x): 0.7067\tD(G(z)): 0.4418 / 0.3013\n",
      "[28/30][100/391]\tLoss_D: 1.0777\tLoss_G: 1.1425\tD(x): 0.6111\tD(G(z)): 0.3310 / 0.3159\n",
      "[28/30][150/391]\tLoss_D: 1.0680\tLoss_G: 1.2300\tD(x): 0.6913\tD(G(z)): 0.4104 / 0.2844\n",
      "[28/30][200/391]\tLoss_D: 1.0781\tLoss_G: 1.1146\tD(x): 0.5379\tD(G(z)): 0.2435 / 0.3272\n",
      "[28/30][250/391]\tLoss_D: 1.0764\tLoss_G: 1.1998\tD(x): 0.6280\tD(G(z)): 0.3602 / 0.2947\n",
      "[28/30][300/391]\tLoss_D: 1.2361\tLoss_G: 0.8148\tD(x): 0.4851\tD(G(z)): 0.3149 / 0.4510\n",
      "[28/30][350/391]\tLoss_D: 1.2103\tLoss_G: 1.0941\tD(x): 0.5942\tD(G(z)): 0.4225 / 0.3305\n",
      "[29/30][0/391]\tLoss_D: 1.0026\tLoss_G: 1.4072\tD(x): 0.7154\tD(G(z)): 0.3767 / 0.2381\n",
      "[29/30][50/391]\tLoss_D: 1.2665\tLoss_G: 0.7785\tD(x): 0.4328\tD(G(z)): 0.2363 / 0.4721\n",
      "[29/30][100/391]\tLoss_D: 1.2190\tLoss_G: 1.1699\tD(x): 0.6324\tD(G(z)): 0.4678 / 0.3056\n",
      "[29/30][150/391]\tLoss_D: 1.0231\tLoss_G: 1.2863\tD(x): 0.6684\tD(G(z)): 0.3638 / 0.2646\n",
      "[29/30][200/391]\tLoss_D: 1.6444\tLoss_G: 2.9005\tD(x): 0.8618\tD(G(z)): 0.7236 / 0.0490\n",
      "[29/30][250/391]\tLoss_D: 1.1883\tLoss_G: 1.5246\tD(x): 0.6658\tD(G(z)): 0.4677 / 0.2072\n",
      "[29/30][300/391]\tLoss_D: 1.0534\tLoss_G: 1.4672\tD(x): 0.7412\tD(G(z)): 0.4241 / 0.2228\n",
      "[29/30][350/391]\tLoss_D: 1.1262\tLoss_G: 1.0033\tD(x): 0.5777\tD(G(z)): 0.3360 / 0.3692\n",
      "[30/30][0/391]\tLoss_D: 1.0890\tLoss_G: 0.8593\tD(x): 0.5833\tD(G(z)): 0.3144 / 0.4354\n",
      "[30/30][50/391]\tLoss_D: 0.9616\tLoss_G: 1.5018\tD(x): 0.7416\tD(G(z)): 0.3711 / 0.2094\n",
      "[30/30][100/391]\tLoss_D: 1.2417\tLoss_G: 1.0137\tD(x): 0.4640\tD(G(z)): 0.2560 / 0.3661\n",
      "[30/30][150/391]\tLoss_D: 1.1167\tLoss_G: 0.9602\tD(x): 0.5090\tD(G(z)): 0.2192 / 0.3859\n",
      "[30/30][200/391]\tLoss_D: 1.1695\tLoss_G: 1.5727\tD(x): 0.7812\tD(G(z)): 0.5238 / 0.2014\n",
      "[30/30][250/391]\tLoss_D: 1.1025\tLoss_G: 1.3324\tD(x): 0.6501\tD(G(z)): 0.3916 / 0.2571\n",
      "[30/30][300/391]\tLoss_D: 1.0936\tLoss_G: 1.4768\tD(x): 0.6970\tD(G(z)): 0.4316 / 0.2158\n",
      "[30/30][350/391]\tLoss_D: 1.0470\tLoss_G: 1.5125\tD(x): 0.7420\tD(G(z)): 0.4285 / 0.2072\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.watch_called = False \n",
    "# WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
    "config = wandb.config          # Initialize config\n",
    "config.batch_size = batch_size \n",
    "config.epochs = num_epochs         \n",
    "config.lr = lr              \n",
    "config.beta1 = beta1\n",
    "config.nz = nz          \n",
    "config.no_cuda = False         \n",
    "config.seed = manualSeed # random seed (default: 42)\n",
    "config.log_interval = 10 # how many batches to wait before logging training status\n",
    "\n",
    "def main():\n",
    "    use_cuda = not config.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    random.seed(config.seed)       # python random seed\n",
    "    torch.manual_seed(config.seed) # pytorch random seed\n",
    "    np.random.seed(config.seed) # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # Load the dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=config.batch_size,\n",
    "                                              shuffle=True, num_workers=workers)\n",
    "\n",
    "    # Create the generator\n",
    "    netG = Generator(ngpu).to(device)\n",
    "\n",
    "    # Handle multi-gpu if desired\n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "    # Apply the weights_init function to randomly initialize all weights\n",
    "    #  to mean=0, stdev=0.2.\n",
    "    netG.apply(weights_init)\n",
    "\n",
    "    # Create the Discriminator\n",
    "    netD = Discriminator(ngpu).to(device)\n",
    "\n",
    "    # Handle multi-gpu if desired\n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "    # Apply the weights_init function to randomly initialize all weights\n",
    "    #  to mean=0, stdev=0.2.\n",
    "    netD.apply(weights_init)\n",
    "\n",
    "    # Initialize BCELoss function\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Setup Adam optimizers for both G and D\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=config.lr, betas=(config.beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=config.lr, betas=(config.beta1, 0.999))\n",
    "    \n",
    "    # WandB – wandb.watch() automatically fetches all layer dimensions, gradients, model parameters and logs them automatically to your dashboard.\n",
    "    # Using log=\"all\" log histograms of parameter values in addition to gradients\n",
    "    wandb.watch(netG, log=\"all\")\n",
    "    wandb.watch(netD, log=\"all\")\n",
    "    iters = 0\n",
    "    for epoch in range(1, config.epochs + 1):\n",
    "        train(config, netG, netD, device, trainloader, optimizerG, optimizerD, criterion, epoch, iters)\n",
    "    # WandB – Save the model checkpoint. This automatically saves a file to the cloud and associates it with the current run.\n",
    "    torch.save(netG.state_dict(), \"model.h5\")\n",
    "    checkpoint = {'state_dict': netD.state_dict(),'optimizer' :optimizerD.state_dict()}\n",
    "    torch.save(checkpoint, f'cifar-discriminator.pt')\n",
    "    wandb.save('model.h5')\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Rj5lG4LGViA"
   },
   "source": [
    "### References\n",
    "1. DCGAN Pytorch Tutorial: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7765ceeab2682aeff90f785cce55ae4b35d72b281a63346ed896e5b68bf02d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
